#slurm profile
cluster:
    mkdir -p logs/{rule}/ &&
    sbatch
        --cpus-per-task={threads}
        --mem={resources.mem_mb}
        --time={resources.time}
        --job-name=smk-{rule}
        --parsable
        --output=logs/{rule}/{jobid}.out
        --error=logs/{rule}/{jobid}.err
       --partition=compute
#       --gres=gpu:{resources.gpu}
default-resources:
  - mem_mb=2000
  - time=480
#  - partition=hpc_general
#  - gpu=0
jobs: 1 
latency-wait: 60
local-cores: 2
#restart-times: 1
max-jobs-per-second: 20
#keep-going: True
rerun-incomplete: True
printshellcmds: True
scheduler: greedy
use-conda: True
conda-frontend: mamba
cluster-status: ./status.py
cluster-cancel: scancel
max-status-checks-per-second: 10
